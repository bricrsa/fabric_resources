{"cells":[{"cell_type":"markdown","source":["### Install and import libaries\n","References (detailed links to functions and examples in each cell):\n","- Semantic Link Labs library: https://github.com/microsoft/semantic-link-labs\n","    - Wiki: https://github.com/microsoft/semantic-link-labs/wiki\n","    - sempy_labs package: https://semantic-link-labs.readthedocs.io/en/latest/sempy_labs.html\n","    - Code Examples: https://github.com/microsoft/semantic-link-labs/wiki/Code-Examples\n","    - Helper Notebooks: https://github.com/microsoft/semantic-link-labs/tree/main/notebooks\n","#\n","- Semantic Link: https://learn.microsoft.com/en-us/fabric/data-science/semantic-link-overview\n","    - Sempy Package: https://learn.microsoft.com/en-us/python/api/semantic-link-sempy/sempy?view=semantic-link-python\n","    - Fabric Functions Code Examples: https://github.com/m-kovalsky/Fabric\n","\n","This notebook utilizes the Semantic Link Labs library to dynamically deploy Power BI semantic models and reports across various workspaces. This specific use case involves maintaining a single master model and report, which are then duplicated across multiple workspaces and Lakehouses to accommodate diverse data sources (e.g., by store). Key benefits include centralized governance, automated deployments, separate source tables (potentially smaller row counts), and reduced need for Row-Level Security (RLS), thereby enhancing performance.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1aa35ad9-bd0d-487a-9815-0cdde352a766"},{"cell_type":"code","source":["# Install the library in a Fabric notebook: https://github.com/microsoft/semantic-link-labs?tab=readme-ov-file#install-the-library-in-a-fabric-notebook\n","\n","%pip install semantic-link-labs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"},"jupyter":{"outputs_hidden":true}},"id":"e980d2b1-d896-4ab0-b1ee-82809f1cc2a2"},{"cell_type":"code","source":["# Import the libraries: https://github.com/microsoft/semantic-link-labs?tab=readme-ov-file#once-installed-run-this-code-to-import-the-library-into-your-notebook\n","\n","import sempy.fabric as fabric\n","import sempy_labs as labs\n","from sempy_labs import migration, directlake, admin, graph\n","from sempy_labs import lakehouse as lake\n","from sempy_labs import report as rep\n","from sempy_labs.tom import connect_semantic_model\n","from sempy_labs.report import ReportWrapper"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"6173e04d-2809-44bf-a72b-3a0040e2daa2"},{"cell_type":"markdown","source":["### Programmatically deploy semantic models and reports with Lakehouse schemas"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"90cd0496-c759-48d9-be5a-a9303fcd5ef9"},{"cell_type":"markdown","source":["1. Deploy semantic model to new workspace and rename\n","2. Update semantic model connection to new Lakehouse\n","3. (Optional) Check semantic model Lakehouse connection\n","4. Update Direct Lake table partition to new schema\n","5. (Optional) Get Tabular Model Scripting Language (TMSL) to confirm lineage\n","6. Clone report to new workspace and rebind to new semantic model\n","7. (Optional) Launch report to preview"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"03804a59-1dd8-412b-91d5-dc2909e4fc93"},{"cell_type":"code","source":["source_dataset = 'SOURCE_DATASET_NAME' # semantic model\n","source_dataset_workspace = 'SOURCE_DATASET_WORKSPACE_NAME'\n","source_report = 'SOURCE_WORKSPACE_NAME'\n","source_report_workspace = 'SOURCE_REPORT_WORKSPACE_NAME'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"a302f8ed-8d2a-4fff-8805-7a18ea08175a"},{"cell_type":"code","source":["# Initialize lists for each parameter\n","\n","target_dataset = 'TARGET_DATASET_NAME'\n","target_workspace = 'TARGET_DATASET_WORKSPACE_NAME'\n","target_lakehouse = 'TARGET_LAKEHOUSE_NAME'\n","target_lakehouse_workspace = 'TARGET_LAKEHOUSE_WORKSPACE_NAME'\n","target_schema = ''\n","target_report = 'TARGET_REPORT_NAME'\n","target_report_workspace = 'TARGET_REPORT_WORKSPACE_NAME'"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"cf109e29-f03b-4271-8731-9ae0d90d113e"},{"cell_type":"code","source":["# Deploy semantic model to new workspace\n","\n","\"\"\"\n","    Function - deploy_semantic_model: https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.html#sempy_labs.deploy_semantic_model \n","        Code Example: https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.html#sempy_labs.deploy_semantic_model\n","\"\"\"\n","\n","import sempy_labs as labs\n","\n","refresh_target_dataset = False\n","overwrite = True\n","\n","    \n","labs.deploy_semantic_model(\n","        source_dataset = source_dataset,\n","        source_workspace = source_dataset_workspace,\n","        target_dataset = target_dataset,\n","        target_workspace = target_workspace,\n","        refresh_target_dataset = refresh_target_dataset,\n","        overwrite = overwrite\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"7b33d9f6-fe4c-4307-b0f7-35ce9106ceb4"},{"cell_type":"code","source":["# Update semantic model connection to new Lakehouse\n","\n","\"\"\"\n","     Function - update_direct_lake_model_lakehouse_connection: https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.directlake.html#sempy_labs.directlake.update_direct_lake_model_lakehouse_connection\n","     sempy_labs.directlake.update_direct_lake_model_connection\n","\"\"\"\n","\n","from sempy_labs.directlake import update_direct_lake_model_connection\n","\n","update_direct_lake_model_connection(\n","         dataset = target_dataset,\n","         workspace =  target_workspace,\n","         source = target_lakehouse,\n","         source_type = 'Lakehouse',\n","         source_workspace = target_lakehouse_workspace,\n","         use_sql_endpoint  = False\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"9a7f7a53-01b9-432c-8035-64c3434ad096"},{"cell_type":"code","source":["# And now refresh it\n","\n","labs.refresh_semantic_model(\n","    dataset = target_dataset,\n","    workspace = target_workspace\n","    )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"cd1b2b1c-c2bc-495e-a08d-c03dbd159b81"},{"cell_type":"markdown","source":["Note: \n","- After initial model deployment: need to update security role members, cloud connection from SSO (default) to fixed identity, access permissions.\n","- Once deployed and model is overwritten only cloud connection needs to be updated."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea2d20ce-641d-48bc-aae7-c77c61e12856"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}