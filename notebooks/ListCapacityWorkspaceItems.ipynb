{"cells":[{"cell_type":"markdown","source":["## Review\n","\n","https://github.com/microsoft/semantic-link-labs/blob/main/notebooks/Capacity%20Migration.ipynb for info on Capacity Migration and ideas\n","\n","1. Attach a lakehouse a schema enabled Lakehouse to the Notebook\n","2. Execute all cells\n","3. Execute again after changes to any Fabric item\n","3. Review changes to SCDs\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"25baea7c-fc23-4f58-b573-e8dc8864dfa2"},{"cell_type":"code","source":["!pip install --upgrade semantic-link --q #upgrade to most recent semantic-link...not necessary for Fabric runtime 1.3"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":false,"run_control":{"frozen":true}},"id":"88dd7108-d877-4f9c-9fbe-03ea6b658746"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","import pandas as pd\n","from pyspark.sql.functions import *\n","from datetime import datetime\n","\n","current_datetime = datetime.now()\n","#change this to hide result of API loads\n","show_results = False"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f24f650-ffcc-4b68-a7ac-2317aee37d4d"},{"cell_type":"code","source":["%%sql\n","CREATE SCHEMA IF NOT EXISTS stage"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"f0611537-5fb4-468a-a491-25f00eee3eb4"},{"cell_type":"markdown","source":["## Fetch data for Capacities, Workspaces and Items from API\n","\n","Using sempy and logged in user"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c635292c-0cc2-4676-b0d2-5b2eeb26bdcd"},{"cell_type":"code","source":["capacity_list = fabric.list_capacities()\n","capacity_list = capacity_list.rename(columns={'Id': 'CapacityId','Display Name': 'CapacityName'})\n","capacity_list['EffectiveDate'] = current_datetime\n","spark_capacity_list=spark.createDataFrame(capacity_list)\n","spark_capacity_list = spark_capacity_list.withColumn('row_checksum', md5(concat_ws('CapacityId','CapacityName','SKU','Region','State')))\n","spark_capacity_list.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"stage.stage_capacity_list\")\n","if show_results:\n","    display(spark_capacity_list)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"1507b9e6-5d6a-4cb3-b270-5c21a9dda998"},{"cell_type":"code","source":["ws_list = fabric.list_workspaces()\n","ws_list = ws_list.rename(columns={'Id': 'WorkspaceId','Display Name': 'CapacityName', 'Capacity Id': 'CapacityId', 'Is Read Only':  'IsReadOnly',\n","            'Is On Dedicated Capacity': 'IsOnDedicatedCapacity','Default Dataset Storage Format': 'DefaultDatasetStorageFormat',\n","            'Name' : 'WorkspaceName'})\n","ws_list['EffectiveDate'] = current_datetime\n","spark_ws_list=spark.createDataFrame(ws_list)\n","spark_ws_list = spark_ws_list.withColumn('row_checksum', md5(concat_ws('WorkspaceId','IsReadOnly','isOnDedicatedCapacity','CapacityId','DefaultDatasetStorageFormat','Type', 'WorkspaceName')))\n","spark_ws_list.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"stage.stage_workspace_list\")\n","if show_results:\n","    display(spark_ws_list)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"1585d97e-b25f-47d9-ac6b-b60183700f47"},{"cell_type":"code","source":["ws_item_list = pd.concat([fabric.list_items(workspace=ws) for ws in fabric.list_workspaces().query('`Is On Dedicated Capacity` == True').Id], ignore_index=True)\n","ws_item_list = ws_item_list.rename(columns={'Id': 'ItemId','Display Name': 'ItemName', 'Workspace Id': 'WorkspaceId'})\n","ws_item_list['EffectiveDate'] = current_datetime\n","spark_ws_item_list=spark.createDataFrame(ws_item_list)\n","spark_ws_item_list = spark_ws_item_list.withColumn('row_checksum', md5(concat_ws('ItemId','ItemName','Description','Type','WorkspaceId')))\n","spark_ws_item_list.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"stage.stage_workspace_item_list\")\n","if show_results:\n","    display(spark_ws_item_list)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"6de97939-5298-4ae6-93f4-cf2f35110b22"},{"cell_type":"markdown","source":["## Create tables for history if they don't exist"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eba4f2df-c5b5-48dd-b516-2a5fd931f519"},{"cell_type":"code","source":["%%sql\n","create table if not exists dbo.capacity_list\n","    (CapacityId\tstring\n","    , CapacityName\tstring\n","    , Sku\tstring\n","    , Region\tstring\n","    , State\tstring\n","    , EffectiveDate\ttimestamp\n","    , row_checksum string\n","    , current_row boolean\n","    , EndDate timestamp\n","    )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"ba843116-8b0f-483b-a853-abf01a52ec5a"},{"cell_type":"code","source":["%%sql\n","create table if not exists dbo.workspace_list\n","    (WorkspaceId\tstring\n","    , IsReadOnly\tboolean\n","    , IsOnDedicatedCapacity\tboolean\n","    , CapacityId\tstring\n","    , DefaultDatasetStorageFormat\tstring\n","    , Type\tstring\n","    , WorkspaceName\tstring\n","    , EffectiveDate\ttimestamp\n","    , row_checksum\tstring\n","    , current_row boolean\n","    , EndDate timestamp\n","    )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"db6e8fc2-b399-4227-999b-9b20d6b71c3d"},{"cell_type":"code","source":["%%sql\n","create table if not exists dbo.workspace_item_list\n","    (ItemId\tstring\n","    , ItemName\tstring\n","    , Description\tstring\n","    , Type\tstring\n","    , WorkspaceId\tstring\n","    , EffectiveDate\ttimestamp\n","    , row_checksum\tstring\n","    , current_row boolean\n","    , EndDate timestamp\n","    )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"87a78976-5d67-44b1-933d-3cebb2feefd0"},{"cell_type":"markdown","source":["## Populate slowly changing dimensions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"55ace929-5f57-4f2b-ba68-340339bb2ea6"},{"cell_type":"code","source":["  %%sql\n","--scd type 2 for capacity list\n","MERGE INTO dbo.capacity_list\n","USING (\n","  SELECT s.CapacityId as mergeKey, s.*\n","  FROM stage.stage_capacity_list s\n","  UNION ALL\n","  SELECT NULL as mergeKey, s.*\n","  FROM stage.stage_capacity_list s JOIN capacity_list t ON s.CapacityId = t.CapacityId \n","  WHERE s.row_checksum <> t.row_checksum and t.current_row = TRUE\n","    ) staged_updates ON capacity_list.CapacityId = mergeKey\n","WHEN MATCHED AND capacity_list.current_row = TRUE AND capacity_list.row_checksum <> staged_updates.row_checksum THEN  \n","  UPDATE SET endDate = staged_updates.EffectiveDate, current_row = FALSE    \n","WHEN NOT MATCHED THEN \n","  INSERT(CapacityId, CapacityName, Sku, Region, State, row_checksum, EffectiveDate,current_row, EndDate) \n","  VALUES(staged_updates.CapacityId, staged_updates.CapacityName, staged_updates.Sku, staged_updates.Region\n","  , staged_updates.State, staged_updates.row_checksum, staged_updates.EffectiveDate, TRUE, make_date(2099,12,31))\n","WHEN NOT MATCHED BY SOURCE and capacity_list.current_row = TRUE THEN \n","    UPDATE SET EndDate = CURRENT_TIMESTAMP(), current_row= false"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"b9b8b1db-55d3-460f-83fe-d009a3dbf9ce"},{"cell_type":"code","source":["%%sql\n","--scd type 2 for workspace list\n","MERGE INTO dbo.workspace_list\n","USING ( \n","  SELECT s.WorkspaceId as mergeKey, s.*\n","  FROM stage.stage_workspace_list s\n","  UNION ALL\n","  SELECT NULL as mergeKey, s.*\n","  FROM stage.stage_workspace_list s JOIN workspace_list t ON s.WorkspaceId = t.WorkspaceId \n","  WHERE s.row_checksum <> t.row_checksum and t.current_row = TRUE\n","  ) staged_updates\n","ON workspace_list.WorkspaceId = mergeKey\n","WHEN MATCHED AND workspace_list.current_row = TRUE AND workspace_list.row_checksum <> staged_updates.row_checksum THEN  \n","  UPDATE SET endDate = staged_updates.EffectiveDate , current_row = FALSE\n","WHEN NOT MATCHED THEN \n","  INSERT(WorkspaceId, IsReadOnly, IsOnDedicatedCapacity, CapacityId, DefaultDatasetStorageFormat, Type, WorkspaceName, EffectiveDate,row_checksum,current_row, EndDate) \n","  VALUES(staged_updates.WorkspaceId, staged_updates.IsReadOnly, staged_updates.IsOnDedicatedCapacity, staged_updates.CapacityId\n","    , staged_updates.DefaultDatasetStorageFormat, staged_updates.Type\n","    , staged_updates.WorkspaceName, staged_updates.EffectiveDate,staged_updates.row_checksum, TRUE, make_date(2099,12,31))\n","WHEN NOT MATCHED BY SOURCE and workspace_list.current_row = TRUE THEN \n","    UPDATE SET EndDate = CURRENT_TIMESTAMP(), current_row= false"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"1d4d3581-a2f7-4c92-9aa9-e6ae8b1dc33e"},{"cell_type":"code","source":["%%sql\n","--scd type 2 for workspace_item_list \n","MERGE INTO dbo.workspace_item_list\n","USING ( \n","  SELECT s.ItemId as mergeKey, s.*\n","  FROM stage.stage_workspace_item_list s\n","  UNION ALL\n","  SELECT NULL as mergeKey, s.*\n","  FROM stage.stage_workspace_item_list s JOIN dbo.workspace_item_list t ON s.ItemId = t.ItemId \n","  WHERE s.row_checksum <> t.row_checksum and t.current_row = TRUE\n","  ) staged_updates\n","ON workspace_item_list.ItemId  = mergeKey\n","WHEN MATCHED AND workspace_item_list.current_row = TRUE AND workspace_item_list.row_checksum <> staged_updates.row_checksum THEN  \n","  UPDATE SET endDate = staged_updates.EffectiveDate, current_row = FALSE\n","WHEN NOT MATCHED THEN \n","  INSERT(ItemId, ItemName, Description, Type, WorkspaceId, EffectiveDate, row_checksum, current_row, EndDate) \n","  VALUES(staged_updates.ItemId, staged_updates.ItemName, staged_updates.Description\n","    , staged_updates.Type, staged_updates.WorkspaceId, staged_updates.EffectiveDate, staged_updates.row_checksum, TRUE, make_date(2099,12,31))\n","WHEN NOT MATCHED BY SOURCE and workspace_item_list.current_row = TRUE THEN \n","    UPDATE SET EndDate = CURRENT_TIMESTAMP(), current_row= false"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"bbd717c5-e68e-4c79-ac39-e67ef47e1b54"},{"cell_type":"code","source":["%%sql\n","select * from dbo.capacity_list\n","where EffectiveDate > dateadd(current_timestamp(), -3)\n","order by CapacityName, EffectiveDate"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"0953f0ad-991d-4364-ab3b-83cd1e21c983"},{"cell_type":"code","source":["%%sql\n","--changes from the last day, note does not include items where workspace is deleted\n","select c.CapacityName, w.WorkspaceName, i.* \n","from dbo.workspace_item_list i\n","    join workspace_list w on i.WorkspaceId = w.WorkspaceId and w.current_row = True\n","    join capacity_list c on w.CapacityId =c.CapacityId and c.current_row = True\n","where ItemId in (\n","    SELECT ItemId FROM dbo.workspace_item_list \n","    where current_row = false and EndDate > date_add(current_timestamp,-1)\n","    )\n","order by ItemId, EffectiveDate"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"cef1ecf1-abfe-43d4-9c48-69265cdb4d40"},{"cell_type":"code","source":["%%sql\n","--show workspaces by capacity\n","select \n","    c.CapacityId, c.CapacityName, c.Region\n","    , w.WorkspaceId, w.WorkspaceName\n","from\n","dbo.capacity_list c\n","left join dbo.workspace_list w on c.CapacityId = w.CapacityId and c.current_row = True and w.current_row = True\n","order by \n","    CapacityName, WorkspaceName\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"8e226635-e213-4f08-bc69-4c997c39640b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"516f5ccd-1ba4-49d8-9718-0588186b9db6","default_lakehouse_name":"AdminInfo","default_lakehouse_workspace_id":"627ddac8-6645-4c8b-9b30-7d7624716215"}}},"nbformat":4,"nbformat_minor":5}