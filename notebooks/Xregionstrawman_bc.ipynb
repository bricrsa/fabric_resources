{"cells":[{"cell_type":"markdown","source":["#### X Region Migration Script"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2e8b4307-0f56-4bd6-adc4-45d117078483"},{"cell_type":"code","source":["%pip install -q semantic-link-labs"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5e44a2e8-f9fe-4555-ad32-f35156cb60b8"},{"cell_type":"code","source":["source_ws = 'GovernanceWs' # the source workspace to be migrated\n","target_ws_suffix = '_XREGION'\n","\n","target_capacity = 'c1pri'\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"92f1ba69-bdc3-4ffd-9f01-9a65552c3427"},{"cell_type":"code","source":["import pandas as pd\n","import datetime\n","import re,json, fnmatch,os\n","import requests, base64\n","import sempy\n","import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException\n","from pyspark.sql import DataFrame\n","from pyspark.sql.functions import col,current_timestamp,lit\n","import sempy_labs as labs\n","from sempy_labs import migration, directlake\n","from sempy_labs import lakehouse as lake\n","from sempy_labs import report as rep\n","from sempy_labs.tom import connect_semantic_model\n","\n","# instantiate the Fabric rest client\n","client = fabric.FabricRestClient()\n","\n","# get the current workspace ID based on the context of where this notebook is run from\n","thisWsId = notebookutils.runtime.context['currentWorkspaceId']\n","thisWsName = notebookutils.runtime.context['currentWorkspaceName']\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"286aa117-c8d6-4ca2-a4e4-f2b4c24f9b9d"},{"cell_type":"markdown","source":["###### Check the status of the target capacity - it needs to be active to git connect/sync\n","TODO get region of source workspace capacity and check that the region is different"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"470baaac-4eec-4c7a-b7be-81143400ecdd"},{"cell_type":"code","source":["# if target capacity name set then obtain the ID. If not set then raise an error \n","if target_capacity != '':\n","    target_capacity_id = labs.resolve_capacity_id(target_capacity)\n","else:\n","    raise ValueError(f\"Pleas specify a target capacity name in the parameter section\")\n","\n","# get the source workspace capacity details\n","source_capacity_id = labs.resolve_workspace_capacity(source_ws)[0]\n","# TODO then get capacity region details from both an check to see they're different\n","\n","\n","print(f'Target capactiy is set to {target_capacity} ({target_capacity_id}). Checking status...')\n","dfC = fabric.list_capacities()\n","dfC_filt = dfC[dfC[\"Id\"] == target_capacity_id]\n","cap_status = dfC_filt['State'].iloc[0]\n","if cap_status == 'Inactive':\n","    raise ValueError(f\"Status of capacity {target_capacity} is {cap_status}. Please resume the capacity and retry\")\n","else:\n","    print(f\"Status of capacity {target_capacity} is {cap_status}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4d1a7f57-0ffb-4d19-916e-3422f53bb075"},{"cell_type":"markdown","source":["###### Create target workspace"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d9b209d-8451-43e0-953d-acbf0ed48638"},{"cell_type":"code","source":["try:\n","    target_ws = source_ws + target_ws_suffix\n","    # create new workspace\n","    print(\"Creating workspace: \" + target_ws + \" in capacity \"+ target_capacity_id +\"...\")\n","    response = fabric.create_workspace(target_ws,target_capacity_id) \n","    new_workspace_id = response\n","    print(\"Created workspace with ID: \" + new_workspace_id)\n","except Exception as error:\n","    errmsg =  \"Failed to create workspace \" +target_ws + \" with capacity ID (\"+ target_capacity_id + \") due to: \"+str(error)\n","    print(errmsg)\n","    #notebookutils.notebook.exit(f'Workspace {target_ws} already exists')\n","    raise ValueError(f\"Workspace {target_ws} already exists. Please use a different workspace name.\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"adb5fe95-304c-4f7c-bcb2-4459e2d8636a"},{"cell_type":"code","source":["#Get git connection details for current workspace\n","gitconnx = labs.get_git_connection(source_ws)\n","\n","#Connect target workspace\n","labs.connect_workspace_to_git(\n","    gitconnx.loc[0, 'Organization Name'],\n","    gitconnx.loc[0, 'Project Name'],\n","    gitconnx.loc[0, 'Repository Name'],\n","    gitconnx.loc[0, 'Branch Name'],\n","    gitconnx.loc[0, 'Directory Name'],\n","    gitconnx.loc[0, 'Git Provider Type'],\n","    target_ws)\n","\n","# Initialise and sync the items into the target workspace\n","commit_hash = labs.initialize_git_connection(branch_to_new_ws)\n","labs.update_from_git(remote_commit_hash=commit_hash,\n","                     conflict_resolution_policy='PreferWorkspace', \n","                     workspace=branch_to_new_ws)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"85e87e4b-9948-4a10-8486-82d42c301998"},{"cell_type":"markdown","source":["###### Update default lakehouses for notebooks"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c5446a9a-7240-43fc-ad1b-b630e700815c"},{"cell_type":"code","source":["for notebook in notebookutils.notebook.list(workspaceId=new_workspace_id):\n","    if notebook.displayName != 'Create Feature Branch':\n","\n","        # Get the current notebook definition\n","        notebook_def = notebookutils.notebook.getDefinition(notebook.displayName,workspaceId=new_workspace_id)\n","        json_payload = json.loads(notebook_def)\n","        \n","        # Check and remove any attached lakehouses\n","        if 'dependencies' in json_payload['metadata'] and 'lakehouse' in json_payload['metadata']['dependencies']:\n","        # Remove all lakehouses\n","            current_lakehouse = json_payload['metadata']['dependencies']['lakehouse']\n","            json_payload['metadata']['dependencies']['lakehouse'] = {}\n","\n","            #Update new notebook definition after removing existing lakehouses and with new default lakehouseId\n","            (notebookutils.notebook.updateDefinition(\n","                        name = notebook.displayName,\n","                        content  = json.dumps(json_payload),  \n","                        defaultLakehouse = current_lakehouse['default_lakehouse_name'],\n","                        defaultLakehouseWorkspace = new_workspace_id,\n","                        workspaceId = new_workspace_id\n","                        )\n","                )\n","            print(f\"Updated notebook {notebook.displayName} with new default lakehouse: {current_lakehouse['default_lakehouse_name']} in {new_workspace_id}\")\n","       "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e9c4bae1-f4ee-4486-8cef-79e069024a7e"},{"cell_type":"markdown","source":["###### Update direct lake model lakehouse connection\n","\n","https://semantic-link-labs.readthedocs.io/en/stable/sempy_labs.directlake.html#sempy_labs.directlake.update_direct_lake_model_lakehouse_connection\n","    \n","TODO: I think MK has a simplified version of this for all direct lake models in the workspace.\n","TODO: For multi-workspace migration need to adapt this to re-connect to all the migrated workspace lakehouses/warehouses as they may not be in the same workspace "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2fba28cf-f423-4013-b7df-07408b9673ad"},{"cell_type":"code","source":["\n","df_datasets = fabric.list_datasets(branch_to_new_ws)\n","\n","# Iterate over each dataset in the dataframe\n","for index, row in df_datasets.iterrows():\n","    # Check if the dataset is not the default semantic model\n","    if not labs.is_default_semantic_model(row['Dataset Name'], fabric.resolve_workspace_id(branch_to_new_ws)):\n","        print('Updating semantic model connection ' + row['Dataset Name'])\n","        labs.directlake.update_direct_lake_model_lakehouse_connection(dataset=row['Dataset Name'], workspace= branch_to_new_ws,lakehouse =labs.directlake.get_direct_lake_source(row['Dataset Name'], workspace= branch_to_new_ws)[1], lakehouse_workspace=branch_to_new_ws)\n","        labs.refresh_semantic_model(dataset=row['Dataset Name'], workspace= branch_to_new_ws)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"91100d49-96e1-4014-bcd5-b15d27b3843e"},{"cell_type":"markdown","source":["##### Rebind reports in new branch workspace\n","\n","Note sure if we still need to do the rebind.\n","\n","https://semantic-link-labs.readthedocs.io/en/latest/sempy_labs.report.html#sempy_labs.report.report_rebind"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a1786862-7b1f-472e-a50c-ccd2924abd73"},{"cell_type":"code","source":["df_reports = fabric.list_reports(workspace=branch_to_new_ws)\n","for index, row in df_reports.iterrows():\n","    #print(row['Name'] + '-' + row['Dataset Id'])\n","    df_datasets = fabric.list_datasets(workspace=branch_to_new_ws)\n","    dataset_name = df_datasets[df_datasets['Dataset ID'] == row['Dataset Id']]['Dataset Name'].values[0]\n","    print(dataset_name)\n","    labs.report.report_rebind(report=row['Name'],dataset=dataset_name, report_workspace=branch_to_new_ws, dataset_workspace=branch_to_new_ws)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3529074-c9fa-4e97-a21e-d1e917647f8c"},{"cell_type":"markdown","source":["##### TODO workspace role assignments and direct shares\n","\n","I have API code to do this but let's rewrite using SLL"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"413c4a81-7d12-4e60-a1a9-83382926f195"},{"cell_type":"markdown","source":["##### TODO update connections e.g. for data pipelines source and sink connections. SLL has functions for this"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d634883-f6e3-477b-ad9b-7e9a7cd9c154"},{"cell_type":"markdown","source":["###### Optional - disconnect from git"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"24ec33df-06f4-40ad-9be5-830e0647a74a"},{"cell_type":"code","source":["# run this if you want to disconnect from git\n","#labs.disconnect_workspace_from_git(target_ws)     \n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"823eddb3-2a22-4e86-a340-6c08ca87a5cb"},{"cell_type":"markdown","source":["#### Code to copy lakehouse and warehouse data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"083a2f9b-884c-40d2-9825-3bbbb562707a"},{"cell_type":"markdown","source":["###### Utility functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb586c48-2f14-4be3-a434-11bcb1f62945"},{"cell_type":"code","source":["def get_lh_object_list(base_path,data_types = ['Tables', 'Files'])->pd.DataFrame:\n","\n","    '''\n","    Function to get a list of tables for a lakehouse\n","    adapted from https://fabric.guru/getting-a-list-of-folders-and-delta-tables-in-the-fabric-lakehouse\n","    This function will return a pandas dataframe containing names and abfss paths of each folder for Files and Tables\n","    '''\n","    #data_types = ['Tables', 'Files'] #for if you want a list of files and tables\n","    #data_types = ['Tables'] #for if you want a list of tables\n","\n","    df = pd.concat([\n","        pd.DataFrame({\n","            'name': [item.name for item in notebookutils.fs.ls(f'{base_path}/{data_type}/')],\n","            'type': data_type[:-1].lower() , \n","            'src_path': [item.path for item in notebookutils.fs.ls(f'{base_path}/{data_type}/')],\n","        }) for data_type in data_types], ignore_index=True)\n","\n","    return df\n","\n","def get_wh_object_list(schema_list,base_path)->pd.DataFrame:\n","\n","    '''\n","    Function to get a list of tables for a warehouse by schema\n","    '''\n","    data_type = 'Tables'\n","    dfs = []\n","\n","    for schema_prefix in schema_list:\n","        if notebookutils.fs.exists(f'{base_path}/{data_type}/{schema_prefix}/'):\n","            items = notebookutils.fs.ls(f'{base_path}/{data_type}/{schema_prefix}/')\n","            if items:  # Check if the list is not empty\n","                df = pd.DataFrame({\n","                    'schema': schema_prefix,\n","                    'name': [item.name for item in items],\n","                    'type': data_type[:-1].lower(),\n","                    'src_path': [item.path for item in items],\n","                })\n","                dfs.append(df)\n","\n","    if dfs:  # Check if the list of dataframes is not empty\n","        df = pd.concat(dfs, ignore_index=True)\n","    else:\n","        df = pd.DataFrame()  # Return an empty dataframe if no dataframes were created\n","\n","    return df\n","\n","def copy_lh_objects(table_list,workspace_src,workspace_tgt,lakehouse_src,lakehouse_tgt,fastcopy=True,usingIDs=False)->pd.DataFrame:\n","    # declare an array to keep the instrumentation\n","    cpresult = []\n","    # loop through all the tables to extract the source path \n","    for table in table_list.src_path:\n","        source = table\n","        destination = source.replace(f'abfss://{workspace_src}', f'abfss://{workspace_tgt}')\n","        if usingIDs:\n","            destination = destination.replace(f'{lakehouse_src}', f'{lakehouse_tgt}')\n","        else:\n","            destination = destination.replace(f'{lakehouse_src}.Lakehouse', f'{lakehouse_tgt}.Lakehouse')\n","        start_time =  datetime.datetime.now()\n","        if notebookutils.fs.exists(destination):\n","             notebookutils.fs.rm(destination, True)\n","        if fastcopy:\n","            # use fastcopy util which is a python wrapper to azcopy\n","            notebookutils.fs.fastcp(source+'/*', destination+'/', True)\n","        else:\n","            notebookutils.fs.cp(source, destination, True)\n","\n","        # recording the timing and add it to the results list\n","        end_time = datetime.datetime.now()\n","        copyreslist = [source, destination, start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),  end_time.strftime(\"%Y-%m-%d %H:%M:%S\"), str((end_time - start_time).total_seconds())]\n","        cpresult.append(copyreslist)\n","    return pd.DataFrame(cpresult,columns =['source--------------------------------------','target--------------------------------------','start------------','end_time------------','elapsed seconds----'])\n","\n","def createDWrecoverypl(ws_id,pl_name = 'Recover_Warehouse_Data_From_DR'):\n","  client = fabric.FabricRestClient()\n","\n","  dfurl= \"v1/workspaces/\"+ ws_id + \"/items\"\n","  payload = { \n","  \"displayName\": pl_name, \n","  \"type\": \"DataPipeline\", \n","  \"definition\": { \n","    \"parts\": [ \n","      { \n","        \"path\": \"pipeline-content.json\", \n","        \"payload\":  \"{
    "properties": {
        "activities": [
            {
                "name": "IterateSchemaTables",
                "type": "ForEach",
                "dependsOn": [],
                "typeProperties": {
                    "items": {
                        "value": "@pipeline().parameters.tablesToCopy",
                        "type": "Expression"
                    },
                    "batchCount": 20,
                    "activities": [
                        {
                            "name": "CopyWarehouseTables",
                            "type": "Copy",
                            "dependsOn": [
                                {
                                    "activity": "Set table",
                                    "dependencyConditions": [
                                        "Succeeded"
                                    ]
                                }
                            ],
                            "policy": {
                                "timeout": "0.12:00:00",
                                "retry": 2,
                                "retryIntervalInSeconds": 30,
                                "secureOutput": false,
                                "secureInput": false
                            },
                            "typeProperties": {
                                "source": {
                                    "type": "DataWarehouseSource",
                                    "queryTimeout": "02:00:00",
                                    "partitionOption": "None",
                                    "datasetSettings": {
                                        "annotations": [],
                                        "linkedService": {
                                            "name": "07a03006_d1b6_4a39_beb1_0bba2aaf5ff7",
                                            "properties": {
                                                "annotations": [],
                                                "type": "DataWarehouse",
                                                "typeProperties": {
                                                    "endpoint": "@pipeline().parameters.lakehouseConnStr",
                                                    "artifactId": "@pipeline().parameters.lakehouseId",
                                                    "workspaceId": "@pipeline().parameters.workspaceId"
                                                }
                                            }
                                        },
                                        "type": "DataWarehouseTable",
                                        "schema": [],
                                        "typeProperties": {
                                            "schema": "dbo",
                                            "table": {
                                                "value": "@concat(concat(item().schema,'_'),item().name)",
                                                "type": "Expression"
                                            }
                                        }
                                    }
                                },
                                "sink": {
                                    "type": "DataWarehouseSink",
                                    "allowCopyCommand": true,
                                    "tableOption": "autoCreate",
                                    "datasetSettings": {
                                        "annotations": [],
                                        "linkedService": {
                                            "name": "0c03123a_d312_46c4_a8e7_5b4cad8f12d7",
                                            "properties": {
                                                "annotations": [],
                                                "type": "DataWarehouse",
                                                "typeProperties": {
                                                    "endpoint": "@pipeline().parameters.warehouseConnStr",
                                                    "artifactId": "@pipeline().parameters.warehouseId",
                                                    "workspaceId": "@pipeline().parameters.workspaceId"
                                                }
                                            }
                                        },
                                        "type": "DataWarehouseTable",
                                        "schema": [],
                                        "typeProperties": {
                                            "schema": "dbo",
                                            "table": {
                                                "value": "@item().name",
                                                "type": "Expression"
                                            }
                                        }
                                    }
                                },
                                "enableStaging": true,
                                "translator": {
                                    "type": "TabularTranslator",
                                    "typeConversion": true,
                                    "typeConversionSettings": {
                                        "allowDataTruncation": true,
                                        "treatBooleanAsNumber": false
                                    }
                                }
                            }
                        },
                        {
                            "name": "Set table",
                            "type": "SetVariable",
                            "dependsOn": [
                                {
                                    "activity": "Set schema",
                                    "dependencyConditions": [
                                        "Succeeded"
                                    ]
                                }
                            ],
                            "policy": {
                                "secureOutput": false,
                                "secureInput": false
                            },
                            "typeProperties": {
                                "variableName": "Tablename",
                                "value": {
                                    "value": "@item().name",
                                    "type": "Expression"
                                }
                            }
                        },
                        {
                            "name": "Set schema",
                            "type": "SetVariable",
                            "dependsOn": [],
                            "policy": {
                                "secureOutput": false,
                                "secureInput": false
                            },
                            "typeProperties": {
                                "variableName": "Schemaname",
                                "value": {
                                    "value": "@item().schema",
                                    "type": "Expression"
                                }
                            }
                        }
                    ]
                }
            }
        ],
        "parameters": {
            "lakehouseId": {
                "type": "string",
                "defaultValue": "0f0f6b7c-1761-41e6-896e-30014f16ff6d"
            },
            "tablesToCopy": {
                "type": "array",
                "defaultValue": [
                    {
                        "schema": "dbo",
                        "name": "Date"
                    },
                    {
                        "schema": "dbo",
                        "name": "Geography"
                    },
                    {
                        "schema": "dbo",
                        "name": "HackneyLicense"
                    },
                    {
                        "schema": "dbo",
                        "name": "Medallion"
                    },
                    {
                        "schema": "dbo",
                        "name": "Time"
                    },
                    {
                        "schema": "dbo",
                        "name": "Trip"
                    },
                    {
                        "schema": "dbo",
                        "name": "Weather"
                    }
                ]
            },
            "workspaceId": {
                "type": "string",
                "defaultValue": "1501143c-272f-4a2f-976a-7e55971e4c2b"
            },
            "warehouseId": {
                "type": "string",
                "defaultValue": "4d1bd951-99de-4bd7-b7bc-71c8f56db411"
            },
            "warehouseConnStr": {
                "type": "string",
                "defaultValue": "72wwbivi2ubejbrtmtaho32b4y-hqkacfjpe4xuvf3kpzkzohsmfm.datawarehouse.fabric.microsoft.com"
            },
            "lakehouseConnStr": {
                "type": "string",
                "defaultValue": "72wwbivi2ubejbrtmtaho32b4y-hqkacfjpe4xuvf3kpzkzohsmfm.datawarehouse.fabric.microsoft.com"
            }
        },
        "variables": {
            "Tablename": {
                "type": "String"
            },
            "Schemaname": {
                "type": "String"
            }
        },
        "lastModifiedByObjectId": "4aa20af7-94bd-4348-bef8-f8cbcd840d51",
        "lastPublishTime": "2024-11-13T15:52:52Z"
    }
}\", \n","        \"payloadType\": \"InlineBase64\" \n","      } \n","    ] \n","  } \n","}   \n","  \n","  response = json.loads(client.post(dfurl,json= payload).content)\n","  return response['id']\n","\n","def getItemId(wks_id,itm_name,itm_type):\n","    df = fabric.list_items(type=None,workspace=wks_id)\n","    #print(df)\n","    if df.empty:\n","        return 'NotExists'\n","    else:\n","        #display(df)\n","        #print(df.query('\"Display Name\"=\"'+itm_name+'\"'))\n","        if itm_type != '':\n","            newdf= df.loc[(df['Display Name'] == itm_name) & (df['Type'] == itm_type)]['Id']\n","        else:\n","            newdf= df.loc[(df['Display Name'] == itm_name)]['Id']  \n","        if newdf.empty:\n","            return 'NotExists'\n","        else:\n","            return newdf.iloc[0]\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"038f141b-95f3-4922-8fd6-f2ea93985b45"},{"cell_type":"markdown","source":["###### Copy lakehouse data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6b2e931f-3f2e-4edc-9253-a82d6a8b5994"},{"cell_type":"code","source":["df_lakehouses = (labs.list_lakehouses(source_ws))\n","display(df_lakehouses)\n","for index, row in df_lakehouses.iterrows():\n","    lh_name= row['Lakehouse Name']\n","    # Gathers the list of recovers tables and source paths to be copied into the lakehouse associated with this notebook \n","    src_path = f'abfss://{source_ws}@onelake.dfs.fabric.microsoft.com/{lh_name}.Lakehouse'\n","\n","    table_list = get_lh_object_list(src_path)\n","    print('The following tables will attempt to be recovered and persisted as tables in the default lakehouse of this notebook...')\n","    display(table_list)\n","\n","    print('Copy Lakehouse Delta tables...')\n","    res = copy_lh_objects(table_list[table_list['type']=='table'],source_ws,target_ws,\n","                        lh_name,lh_name,False,False)\n","    display(res)\n","    # Copy files\n","    print('Copy Lakehouse files...')\n","    res = copy_lh_objects(table_list[table_list['type']=='file'],source_ws,target_ws,\n","                        lh_name,lh_name,False,False)\n","    display(res)\n","    print('Done.')\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aed6bed8-a6ab-4da0-b4ef-e831989427c6"},{"cell_type":"markdown","source":["###### Copy warehouse data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4764510b-0f4f-406b-8fa2-7eb29bb8c86d"},{"cell_type":"code","source":["p_logging_verbose = True\n","source_ws_id = fabric.resolve_workspace_id(source_ws)\n","target_ws_id = fabric.resolve_workspace_id(target_ws)\n","df_warehouses = (labs.list_warehouses(source_ws))\n","display(df_warehouses)\n","for index, row in df_warehouses.iterrows():\n","    source_wh_id = labs.resolve_warehouse_id(row['Warehouse Name'],source_ws_id)\n","    target_wh_id = labs.resolve_warehouse_id(row['Warehouse Name'],target_ws_id)\n","    \n","    src_path = f'abfss://'+source_ws_id+'@onelake.dfs.fabric.microsoft.com/'+source_wh_id\n","    tgt_path = f'abfss://'+target_ws_id+'@onelake.dfs.fabric.microsoft.com/'+target_wh_id\n","\n","    # extract the list of schemas per data \n","    schema_list = get_lh_object_list(src_path,['Tables'])\n","    # extract a list of warehouse objects per schema and store in a list\n","    table_list = get_wh_object_list(schema_list['name'],src_path)\n","  \n","    # create a temporary staging lakehouse per warehouse to create shortcuts into, \n","    # which point back to original warehouse data currently in the DR storage account\n","    lhname = 'temp_rlh_' + source_ws+'_'+row['Warehouse Name']\n","    # check if it exists before attempting create\n","    if p_logging_verbose:\n","        print('Checking whether the temporary lakehouse \"'+ lhname +'\" exists in workspace '+target_ws+'...')\n","    temp_lh_id = getItemId(target_ws_id,lhname,'Lakehouse')\n","    if temp_lh_id == 'NotExists':\n","        lhname = lhname[:256] # lakehouse name should not exceed 256 characters\n","        payload = payload = '{\"displayName\": \"' + lhname + '\",' \\\n","        + '\"description\":  \"Interim staging lakehouse for primary warehouse recovery: ' \\\n","        + source_ws+'_'+row['Warehouse Name'] + 'into workspace '+ target_ws + '(' + target_ws +')\"}'\n","        try:\n","            lhurl = \"v1/workspaces/\" + target_ws_id + \"/lakehouses\"\n","            lhresponse = client.post(lhurl,json= json.loads(payload))\n","            temp_lh_id = lhresponse.json()['id']\n","            if p_logging_verbose:\n","                print('Temporary lakehouse \"'+ lhname +'\" created with Id ' + temp_lh_id + ': ' + str(lhresponse.status_code) + ' ' + str(lhresponse.text))\n","        except Exception as error:\n","            print(error.errorCode)\n","    else:\n","        if p_logging_verbose:\n","            print('Temporary lakehouse '+lhname+' (' + temp_lh_id + ') already exists.')\n","        \n","\n","    # Create shortcuts for every table in the format of schema_table under the tables folder\n","    for index,itable in table_list.iterrows():\n","        shortcutExists=False\n","        # Check if shortcut exists\n","        try:\n","            url = \"v1/workspaces/\" + target_ws_id + \"/items/\" + temp_lh_id + \"/shortcuts/Tables/\"+itable['schema']+'_'+itable['name']\n","            tlhresponse = client.get(url)\n","            shortcutExists = True\n","            if p_logging_verbose:\n","                print('Shortcut '+itable['schema']+'_'+itable['name'] +' already exists')\n","        except Exception as error:\n","            shortcutExists = False    \n","\n","        if not shortcutExists: \n","            # Create shortcuts - one per table per schema\n","            url = \"v1/workspaces/\" + target_ws_id + \"/items/\" + temp_lh_id + \"/shortcuts\"\n","            scpayload = '{' \\\n","            '\"path\": \"Tables/\",' \\\n","            '\"name\": \"'+itable['schema']+'_'+itable['name']+'\",' \\\n","            '\"target\": {' \\\n","            '\"oneLake\": {' \\\n","                '\"workspaceId\": \"' + source_ws_id + '\",' \\\n","                '\"itemId\": \"'+ source_wh_id +'\",' \\\n","                '\"path\": \"/Tables/' + itable['schema']+'/'+itable['name'] + '\"' \\\n","                '}}}' \n","            try:\n","                #print(scpayload)                \n","                shctresponse = client.post(url,json= json.loads(scpayload))\n","                if p_logging_verbose:\n","                    print('Shortcut '+itable['schema']+'_'+itable['name'] + ' created.' )\n","\n","            except Exception as error:\n","                print('Error creating shortcut '+itable['schema']+'_'+itable['name']+' due to '+str(error) + ':' + shctresponse.text)\n","    \n","    recovery_pipeline_prefix= 'plRecover_WH'       \n","    # recovery pipeline name should not exceed 256 characters\n","    recovery_pipeline = recovery_pipeline_prefix+'_'+source_ws + '_'+row['Warehouse Name'][:256]\n","    if p_logging_verbose:\n","        print('Attempting to deploy a copy pipeline in the target workspace to load the target warehouse tables from the shortcuts created above... ')\n","    # Create the pipeline in the target workspace that loads the target warehouse from shortcuts created above \n","    plid = getItemId( target_ws_id,recovery_pipeline,'DataPipeline')\n","    #print(plid)\n","    if plid == 'NotExists':\n","      plid = createDWrecoverypl(target_ws_id,recovery_pipeline_prefix+'_'+source_ws + '_'+row['Warehouse Name'])\n","      if p_logging_verbose:\n","          print('Recovery pipeline ' + recovery_pipeline + ' created with Id '+plid)\n","    else:\n","      if p_logging_verbose:\n","          print('Datawarehouse recovery pipeline \"' + recovery_pipeline + '\" ('+plid+') already exist in workspace \"'+target_ws + '\" ('+target_ws_id+')')  \n","          print('\\n')\n","\n","    tablesToCopyParam = table_list[['schema','name']].to_json( orient='records')\n","    # ensure the temporary lakehouse exists\n","\n","    # obtain the connection string for the lakehouse to pass to the copy pipeline\n","    whurl  = \"v1/workspaces/\" + target_ws_id + \"/lakehouses/\" + temp_lh_id\n","    whresponse = client.get(whurl)\n","    lhconnStr = whresponse.json()['properties']['sqlEndpointProperties']['connectionString']\n","\n","    # get the SQLEndpoint ID of the lakehouse to pass to the copy pipeline\n","    items = fabric.list_items(workspace=target_ws_id)\n","    temp_lh_sqle_id = items[(items['Type'] == 'SQLEndpoint') & (items['Display Name']==lhname)]['Id'].values[0]\n","\n","\n","    # obtain the connection string for the warehouse to pass to the copy pipeline    \n","    whurl  = \"v1/workspaces/\" + target_ws_id + \"/warehouses/\" + target_wh_id\n","    whresponse = client.get(whurl)\n","    whconnStr = whresponse.json()['properties']['connectionInfo']\n","\n","    # obtain the pipeline id created to recover this warehouse\n","    plid = getItemId( i['secondary_ws_id'],recovery_pipeline,'DataPipeline')\n","    if plid == 'NotExists':\n","        print('Error: Could not execute pipeline '+recovery_pipeline+ ' as the ID could not be obtained ')\n","    else:\n","        # pipeline url including pipeline Id unique to each warehouse\n","        plurl = 'v1/workspaces/'+target_ws_id+'/items/'+plid+'/jobs/instances?jobType=Pipeline'\n","        #print(plurl)\n","\n","        payload_data = '{' \\\n","            '\"executionData\": {' \\\n","                '\"parameters\": {' \\\n","                    '\"lakehouseId\": \"' + temp_lh_sqle_id + '\",' \\\n","                    '\"tablesToCopy\": ' + tablesToCopyParam + ',' \\\n","                    '\"workspaceId\": \"' + i['secondary_ws_id'] +'\",' \\\n","                    '\"warehouseId\": \"' + i['secondary_id'] + '\",' \\\n","                    '\"lakehouseConnStr\": \"' + lhconnStr + '\",' \\\n","                    '\"warehouseConnStr\": \"' + whconnStr + '\"' \\\n","                    '}}}'\n","        print(payload_data)\n","        plresponse = client.post(plurl, json=json.loads(payload_data))\n","        if p_logging_verbose:\n","            print(str(plresponse.status_code))      \n","print('Done')\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b0f9abc9-fa09-4a17-a569-59a9f66b3059"},{"cell_type":"markdown","source":["Note that warehouse recovery requires the intermediate step of copying to a Lakehouse before pipelining into warehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f63c8554-856d-447e-a318-246e8d6bc657"},{"cell_type":"code","source":["print('Starting warehouse recovery pipelines...')\n","# interate through all the data warehouses to recover\n","for idx,i in enumerate(warehousedf):\n","    if p_logging_verbose:\n","        print('Invoking pipeline to copy warehouse data from  '+i['primary_ws_name'] + '.'+i['primary_name'] + ' into ' + i['secondary_ws_name'] + '.'+i['secondary_name'] )\n","\n","    src_path = f'abfss://'+i['primary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['primary_id']\n","    #tgt_path = f'abfss://'+i['secondary_ws_id']+'@onelake.dfs.fabric.microsoft.com/'+i['secondary_id']\n","\n","    # extract the list of schemas per data \n","    schema_list = get_lh_object_list(src_path,['Tables'])\n","    #display(schema_list)\n","    # extract a list of warehouse objects per schema and store in a list\n","    table_list = get_wh_object_list(schema_list['name'].to_list(),src_path)\n","\n","    tablesToCopyParam = table_list[['schema','name']].to_json( orient='records')\n","    # ensure the temporary lakehouse exists\n","    lhname = 'temp_rlh_' + i['primary_ws_name']+'_'+i['primary_name']\n","    temp_lh_id = getItemId(i['secondary_ws_id'],lhname,'Lakehouse')\n","    #temp_lh_id ='0f0f6b7c-1761-41e6-896e-30014f16ff6d'\n","    \n","    # obtain the connection string for the lakehouse to pass to the copy pipeline\n","    whurl  = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/lakehouses/\" + temp_lh_id\n","    whresponse = client.get(whurl)\n","    lhconnStr = whresponse.json()['properties']['sqlEndpointProperties']['connectionString']\n","\n","    # get the SQLEndpoint ID of the lakehouse to pass to the copy pipeline\n","    items = fabric.list_items(workspace=i['secondary_ws_id'])\n","    temp_lh_sqle_id = items[(items['Type'] == 'SQLEndpoint') & (items['Display Name']==lhname)]['Id'].values[0]\n","\n","\n","    # obtain the connection string for the warehouse to pass to the copy pipeline    \n","    whurl  = \"v1/workspaces/\" + i['secondary_ws_id'] + \"/warehouses/\" + i['secondary_id']\n","    whresponse = client.get(whurl)\n","    whconnStr = whresponse.json()['properties']['connectionInfo']\n","\n","    recovery_pipeline = recovery_pipeline_prefix+'_'+i['primary_ws_name'] + '_'+i['primary_name'][:256]\n","    # obtain the pipeline id created to recover this warehouse\n","    plid = getItemId( i['secondary_ws_id'],recovery_pipeline,'DataPipeline')\n","    if plid == 'NotExists':\n","        print('Error: Could not execute pipeline '+recovery_pipeline+ ' as the ID could not be obtained ')\n","    else:\n","        # pipeline url including pipeline Id unique to each warehouse\n","        plurl = 'v1/workspaces/'+i['secondary_ws_id'] +'/items/'+plid+'/jobs/instances?jobType=Pipeline'\n","        #print(plurl)\n","\n","        payload_data = '{' \\\n","            '\"executionData\": {' \\\n","                '\"parameters\": {' \\\n","                    '\"lakehouseId\": \"' + temp_lh_sqle_id + '\",' \\\n","                    '\"tablesToCopy\": ' + tablesToCopyParam + ',' \\\n","                    '\"workspaceId\": \"' + i['secondary_ws_id'] +'\",' \\\n","                    '\"warehouseId\": \"' + i['secondary_id'] + '\",' \\\n","                    '\"lakehouseConnStr\": \"' + lhconnStr + '\",' \\\n","                    '\"warehouseConnStr\": \"' + whconnStr + '\"' \\\n","                    '}}}'\n","        print(payload_data)\n","        plresponse = client.post(plurl, json=json.loads(payload_data))\n","        if p_logging_verbose:\n","            print(str(plresponse.status_code))      \n","print('Done')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6a82305b-ea5b-4a73-a8f8-2fb1f9296e16"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"widgets":{},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}