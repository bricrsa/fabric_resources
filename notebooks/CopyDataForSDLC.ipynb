{"cells":[{"cell_type":"markdown","source":["# Move data appropriately from one Lakehouse to another to support deployment and testing"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"386300b9-e046-4179-bc4d-dbe18ef700e9"},{"cell_type":"code","source":["from notebookutils import mssparkutils\n","import pandas as pd\n","import datetime\n","\n","source_workspace = 'LandRegistry'\n","target_workspace = 'LandRegistry_UAT'\n","\n","source_lakehouse = 'PPDOneLake'\n","target_lakehouse = source_lakehouse\n","\n","base_path = f'abfss://{source_workspace}@onelake.dfs.fabric.microsoft.com/{source_lakehouse}.Lakehouse'"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cdbad4ce-c5aa-493d-bbb5-53294461ba7b"},{"cell_type":"markdown","source":["# Set up functions"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7cdd3151-8d1c-4044-ab90-c7163ffc6586"},{"cell_type":"code","source":["def get_file_table_list(base_path)->pd.DataFrame:\n","\n","    '''\n","    Function to get a list of tables for a lakehouse\n","    adapted from https://fabric.guru/getting-a-list-of-folders-and-delta-tables-in-the-fabric-lakehouse\n","    This function will return a pandas dataframe containing names and abfss paths of each folder for Files and Tables\n","    '''\n","    data_types = ['Tables', 'Files'] #for if you want a list of files and tables\n","    #data_types = ['Tables'] #for if you want a list of tables\n","\n","    df = pd.concat([\n","        pd.DataFrame({\n","            'name': [item.name for item in mssparkutils.fs.ls(f'{base_path}/{data_type}/')],\n","            'type': data_type[:-1].lower() , \n","            'path': [item.path for item in mssparkutils.fs.ls(f'{base_path}/{data_type}/')],\n","        }) for data_type in data_types], ignore_index=True)\n","\n","    return df"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7e54479a-b9a4-48ea-9f83-e7f64ba73dc5"},{"cell_type":"markdown","source":["Copy the tables one by one. Use a naive replace to insert target locations.\n","\n","For more fancyness like copying to a certain timestamp, <br>\n","review https://learn.microsoft.com/en-us/fabric/security/experience-specific-guidance?source=recommendations#approach-1-using-custom-script-to-copy-lakehouse-delta-tables-and-files <br>\n","and https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#delete-file-or-directory"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1fe5f1fb-b19c-412b-9c61-3411b05b7ed8"},{"cell_type":"code","source":["def copy_tables(table_list):\n","    print (f'source, target, start time, end time, elapsed')\n","    for table in table_list.path:\n","        source = table\n","        destination = source.replace(f'abfss://{source_workspace}', f'abfss://{target_workspace}')\n","        destination = destination.replace(f'{source_lakehouse}.Lakehouse', f'{target_lakehouse}.Lakehouse')\n","        start_time =  datetime.datetime.now()\n","        mssparkutils.fs.cp(source, destination, True)\n","        end_time = datetime.datetime.now()\n","        print (f'{source}, {destination}, ',  start_time.strftime(\"%Y-%m-%d %H:%M:%S\"),  end_time.strftime(\"%Y-%m-%d %H:%M:%S\"), end_time - start_time)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"808e1ee6-8580-485e-bca5-0861eed415bd"},{"cell_type":"markdown","source":["# Get List of Lakehouse tables and files\n","You need to copy them one by one"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d831d378-0a48-41ce-b460-23c54b4086ff"},{"cell_type":"code","source":["table_list = get_file_table_list(base_path)\n","display(table_list)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e6bbd139-eb5a-411a-9203-9f97feb4f5fd"},{"cell_type":"markdown","source":["### Copy Tables"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd09fd8a-6ecb-4626-8a59-6ecfd5e2601f"},{"cell_type":"code","source":["copy_tables(table_list[table_list['type']=='table'])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"51fa3a16-85fe-42a5-b791-6b3d3abba203"},{"cell_type":"markdown","source":["### Copy Files"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70a4e455-4c1e-49c2-b28d-2588934f51f2"},{"cell_type":"code","source":["copy_tables(table_list[table_list['type']=='file'])"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"66a57dea-1a8b-49ab-ab7e-d794d4f4e524"},{"cell_type":"markdown","source":["### Check copied data"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c4464eb9-a288-40eb-b751-d096c0587eab"},{"cell_type":"code","source":["updated_target_list = get_file_table_list(f'abfss://{target_workspace}@onelake.dfs.fabric.microsoft.com/{target_lakehouse}.Lakehouse')\n","display(updated_target_list)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9e951771-e1ea-4df3-a1ef-fe26e0382ae9"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}